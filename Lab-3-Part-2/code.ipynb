{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np, pandas as pd\n",
    "import random\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForeignChars(token: str):\n",
    "    for letter in set(token):\n",
    "        if ord(letter) not in range(65, 65+26) and ord(letter) not in range(97, 97+26):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Text Dataset\n",
    "    Text Dataset Class\n",
    "    \n",
    "    This class is in charge of managing text data as vectors\n",
    "    Data is saved as vectors (not as text)\n",
    "    Attributes\n",
    "    ----------\n",
    "    seq_length - int: Sequence length\n",
    "    chars - list(str): List of characters\n",
    "    char_to_idx - dict: dictionary from character to index\n",
    "    idx_to_char - dict: dictionary from index to character\n",
    "    vocab_size - int: Vocabulary size\n",
    "    data_size - int: total length of the text\n",
    "    \"\"\"\n",
    "    def __init__(self, text_data: str, seq_length: int = 25) -> None:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        text_data: Full text data as string\n",
    "        seq_length: sequence length. How many characters per index of the dataset.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(list(set(text_data)))\n",
    "        self.data_size, self.vocab_size = len(text_data), len(self.chars)\n",
    "        # useful way to fetch characters either by index or char\n",
    "        self.idx_to_char = {i:ch for i, ch in enumerate(self.chars)}\n",
    "        self.char_to_idx = {ch:i for i, ch in enumerate(self.chars)}\n",
    "        self.seq_length = seq_length\n",
    "        self.X = self.string_to_vector(text_data)\n",
    "    \n",
    "    @property\n",
    "    def X_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns X in string form\n",
    "        \"\"\"\n",
    "        return self.vector_to_string(self.X)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        We remove the last sequence to avoid conflicts with Y being shifted to the left\n",
    "        This causes our model to never see the last sequence of text\n",
    "        which is not a huge deal, but its something to be aware of\n",
    "        \"\"\"\n",
    "        return int(len(self.X) / self.seq_length -1)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        X and Y have the same shape, but Y is shifted left 1 position\n",
    "        \"\"\"\n",
    "        start_idx = index * self.seq_length\n",
    "        end_idx = (index + 1) * self.seq_length\n",
    "\n",
    "        X = torch.tensor(self.X[start_idx:end_idx]).float()\n",
    "        y = torch.tensor(self.X[start_idx+1:end_idx+1]).float()\n",
    "        return X, y\n",
    "    \n",
    "    def string_to_vector(self, name: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Converts a string into a 1D vector with values from char_to_idx dictionary\n",
    "        Inputs\n",
    "        name: Name as string\n",
    "        Outputs\n",
    "        name_tensor: name represented as list of integers (1D vector)\n",
    "        sample:\n",
    "        >>> string_to_vector('test')\n",
    "        [20, 5, 19, 20]\n",
    "        \"\"\"\n",
    "        vector = list()\n",
    "        for s in name:\n",
    "            vector.append(self.char_to_idx[s])\n",
    "        return vector\n",
    "\n",
    "    def vector_to_string(self, vector: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Converts a 1D vector into a string with values from idx_to_char dictionary\n",
    "        Inputs\n",
    "        vector: 1D vector with values in the range of idx_to_char\n",
    "        Outputs\n",
    "        vector_string: Vector converted to string\n",
    "        sample:\n",
    "        >>> vector_to_string([20, 5, 19, 20])\n",
    "        'test'\n",
    "        \"\"\"\n",
    "        vector_string = \"\"\n",
    "        for i in vector:\n",
    "            vector_string += self.idx_to_char[i]\n",
    "        return vector_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic RNN block. This represents a single layer of RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        \"\"\"\n",
    "        input_size: Number of features of your input vector\n",
    "        hidden_size: Number of hidden neurons\n",
    "        output_size: Number of features of your output vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns computed output and tanh(i2h + h2h)\n",
    "        Inputs\n",
    "        ------\n",
    "        x: Input vector\n",
    "        hidden_state: Previous hidden state\n",
    "        Outputs\n",
    "        -------\n",
    "        out: Linear output (without activation because of how pytorch works)\n",
    "        hidden_state: New hidden state matrix\n",
    "        \"\"\"\n",
    "        x = self.i2h(x)\n",
    "        hidden_state = self.h2h(hidden_state)\n",
    "        hidden_state = torch.tanh(x + hidden_state)\n",
    "        out = self.h2o(hidden_state)\n",
    "        return out, hidden_state\n",
    "        \n",
    "    def init_zero_hidden(self, batch_size=1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "\t\t\t\tHelper function.\n",
    "        Returns a hidden state with specified batch size. Defaults to 1\n",
    "        \"\"\"\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: RNN, dataset: TextDataset, prediction_length: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Generate text up to prediction_length characters\n",
    "    This function requires the dataset as argument in order to properly\n",
    "    generate the text and return the output as strings\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predicted = dataset.vector_to_string([random.randint(0, len(dataset.chars) -1)])\n",
    "    hidden = model.init_zero_hidden()\n",
    "\n",
    "    for i in range(prediction_length - 1):\n",
    "        last_char = torch.Tensor([dataset.char_to_idx[predicted[-1]]])\n",
    "        X, hidden = last_char.to(device), hidden.to(device)\n",
    "        out, hidden = model(X, hidden)\n",
    "        result = torch.multinomial(nn.functional.softmax(out, 1), 1).item()\n",
    "        #result = out.argmax().item()\n",
    "        predicted += dataset.idx_to_char[result]\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: RNN, data: DataLoader, epochs: int, optimizer: optim.Optimizer, loss_fn: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model for the specified number of epochs\n",
    "    Inputs\n",
    "    ------\n",
    "    model: RNN model to train\n",
    "    data: Iterable DataLoader\n",
    "    epochs: Number of epochs to train the model\n",
    "    optiimizer: Optimizer to use for each epoch\n",
    "    loss_fn: Function to calculate loss\n",
    "    \"\"\"\n",
    "    train_losses = {}\n",
    "    model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"=> Starting training\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = list()\n",
    "        for X, Y in data:\n",
    "            # skip batch if it doesnt match with the batch_size\n",
    "            if X.shape[0] != model.batch_size:\n",
    "                continue\n",
    "            hidden = model.init_zero_hidden(batch_size=model.batch_size)\n",
    "\n",
    "            # send tensors to device\n",
    "            X, Y, hidden = X.to(device), Y.to(device), hidden.to(device)\n",
    "\n",
    "            # 2. clear gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            loss = 0\n",
    "            for c in range(X.shape[1]):\n",
    "                out, hidden = model(X[:, c].reshape(X.shape[0],1), hidden)\n",
    "                l = loss_fn(out, Y[:, c].long())\n",
    "                loss += l\n",
    "\n",
    "            # 4. Compte gradients gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 5. Adjust learnable parameters\n",
    "            # clip as well to avoid vanishing and exploding gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_losses.append(loss.detach().item() / X.shape[1])\n",
    "\n",
    "        train_losses[epoch] = torch.tensor(epoch_losses).mean()\n",
    "        print(f'=> epoch: {epoch + 1}, loss: {train_losses[epoch]}')\n",
    "        print(generate_text(model, data.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: RNN, data: DataLoader, epochs: int, optimizer: optim.Optimizer, loss_fn: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model for the specified number of epochs\n",
    "    Inputs\n",
    "    ------\n",
    "    model: RNN model to train\n",
    "    data: Iterable DataLoader\n",
    "    epochs: Number of epochs to train the model\n",
    "    optiimizer: Optimizer to use for each epoch\n",
    "    loss_fn: Function to calculate loss\n",
    "    \"\"\"\n",
    "    train_losses = {}\n",
    "    model.to('cuda')\n",
    "    \n",
    "    model.train()\n",
    "    print(\"=> Starting training\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = list()\n",
    "        for X, Y in data:\n",
    "            # skip last batch if it doesnt match with the batch_size\n",
    "            if X.shape[0] != model.batch_size:\n",
    "                continue\n",
    "            hidden = model.init_zero_hidden(batch_size=model.batch_size)\n",
    "            # send tensors to device\n",
    "            X, Y, hidden = X.to(device), Y.to(device), hidden.to(device)\n",
    "            # 2. clear gradients\n",
    "            model.zero_grad()\n",
    "            loss = 0\n",
    "            for c in range(X.shape[1]):\n",
    "                out, hidden = model(X[:, c].reshape(X.shape[0],1), hidden)\n",
    "                l = loss_fn(out, Y[:, c].long())\n",
    "                loss += l\n",
    "            # 4. Compte gradients\n",
    "            loss.backward()\n",
    "            # 5. Adjust learnable parameters\n",
    "            # clip as well to avoid vanishing and exploding gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_losses.append(loss.detach().item() / X.shape[1])\n",
    "        train_losses[epoch] = torch.tensor(epoch_losses).mean()\n",
    "        print(f'=> epoch: {epoch + 1}, loss: {train_losses[epoch]}')\n",
    "\t\t\t\t# after each epoch generate text\n",
    "        print(generate_text(model, data.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Starting training\n",
      "=> epoch: 1, loss: 3.8101320266723633\n",
      "ap muy ts mhny lmy r  sy mst lrvsr g bmgtithti g l is mlgn  rerlme zb fe ememv r e lte rr cecleomevc\n",
      "=> epoch: 2, loss: 3.1690242290496826\n",
      "toer fti ueecerttut bei aaarue ke tiee clamdmg cleee ee sha fe  metee somo se be the lss  im htlee l\n",
      "=> epoch: 3, loss: 3.1268882751464844\n",
      "ibr feeacoe lee mecx aahaamme thz ii mre eaeaer sey l ieisndr rah madknevacones rhneasloner lemigdee\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('./IMDB Dataset.csv').iloc[:1001]\n",
    "\n",
    "    updated_reviews = []\n",
    "    restricted = set(stopwords.words())\n",
    "\n",
    "    for r in data['review'].to_list():\n",
    "        split = r.split()\n",
    "        # Append cleaned sentence (as a list of words) to updated_reviews\n",
    "        updated_reviews.append([s.lower() for s in split if not checkForeignChars(s) and len(s) > 1 and s not in restricted])\n",
    "\n",
    "    data['review'] = [' '.join(review) for review in updated_reviews]\n",
    "    rev = data['review'].str.lower().str.strip('.').str.cat(sep=' ')\n",
    "\n",
    "    # Data size variables\n",
    "    seq_length = 25\n",
    "    batch_size = 64\n",
    "    hidden_size = 256\n",
    "\n",
    "    text_dataset = TextDataset(rev, seq_length=seq_length)\n",
    "    text_dataloader = DataLoader(text_dataset, batch_size)\n",
    "\n",
    "    # Model\n",
    "    rnnModel = RNN(1, hidden_size, len(text_dataset.chars)) # 1 because we enter a single number/letter per step.\n",
    "\n",
    "    # Train variables\n",
    "    epochs = 1000\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(rnnModel.parameters(), lr = 0.01)\n",
    "\n",
    "    train(rnnModel, text_dataloader, epochs, optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
